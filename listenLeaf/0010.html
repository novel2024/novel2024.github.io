<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/5.1.0/css/bootstrap.min.css">
</head>
<body>
<div class="container mt-5">
    
    <p> David Geary, a professor of psychology at the University of Missouri, makes a useful distinction between “biologically primary” and “biologically secondary” abilities. Human beings, he points out, are born ready to learn certain things: how to speak the language of the local community, how to find their way around a familiar landscape, how to negotiate the challenges of small-group living. We are not born to learn the intricacies of calculus or the counterintuitive rules of physics; we did not evolve to understand the workings of the financial markets or the complexities of global climate change. And yet we dwell in a world where such biologically secondary capacities hold the key to advancement, even survival.</p>
    
    <p> The demands of the modern environment have now met, and exceeded, the limits of the biological brain.
For a time, it’s true, humanity was able to keep up with its own ever- advancing culture, resourcefully finding ways to use the biological brain better.
As their everyday environments grew more intellectually demanding, people responded by upping their cognitive game. Continual engagement with the mental rigors of modern life—along with improving nutrition, rising living conditions, and reduced exposure to infectious disease and other pathogens— produced a century-long climb in average IQ score, as measured by intelligence tests taken by people all over the globe.</p>
    
    <p> But this upward trajectory is now leveling off. In recent years, IQ scores have stopped rising, or have even begun to drop, in countries like Finland, Norway, Denmark, Germany, France, and Britain. Some researchers suggest that we have now pushed our mental equipment as far as it can go. It may be that “our brains are already working at near-optimal capacity,” note Nicholas Fitz and Peter Reiner, writing in the journal Nature.</p>
    
    <p> Efforts to wrest more intelligence from this organ, they add,
“bump up against the hard limits of neurobiology.” As if to protest this unwelcome truth, attempts to subvert such limits have received growing attention in recent years. Commercial brain-training regimens like Cogmed, Lumosity, and BrainHQ have attracted many who desire to improve their memory and increase their focus; Lumosity alone claims 100 million registered users in 195 countries. At the same time, so-called neuroenhancement—innovations like “smart pills” and electrical brain stimulation that claim to make their users more intelligent—have drawn breathless media coverage, as well as extensive investment from pharmaceutical and biotechnology companies.</p>
    
    <p>
So far, however, these approaches have yielded little more than disappointment and dashed hopes. A team of scientists who set out to evaluate all the peer-reviewed intervention studies cited on the websites of leading brain- training companies could find “little evidence” within those studies “that training improves everyday cognitive performance.” Engaging in brain training does improve users’ performance—but only on exercises highly similar to the ones they’ve been practicing. The effect does not seem to transfer to real-life activities involving attention and memory.</p>
    
    <p> A 2019 study of Cogmed concluded that such transfer “is rare, or possibly inexistent.” A 2017 study of Lumosity determined that “training appears to have no benefits in healthy young adults”; similarly dismal results have been reported for older individuals. In 2016, Lumosity was forced to pay a $2 million fine for deceptive advertising to the US Federal Trade Commission. Smart pills haven’t fared much better; a clinical trial of one “nootropic” drug popular among Silicon Valley tech workers found that a cup of coffee was more effective at boosting memory and attention.</p>
    
    <p>
Medications and technologies that might, someday, actually enhance intelligence remain in the early stages of laboratory testing. The best way—and,
at least for now, the only way—for us to get smarter is to get better at thinking outside the brain. Yet we dismiss or disparage this kind of cognition, to the extent that we consider it at all. Our pronounced bias in favor of brainbound thinking is long-standing and well entrenched—but a bias is all it is, and one that can no longer be supported or sustained.</p>
    
    <p> The future lies in thinking outside the brain.
 WE CAN BETTER grasp the future of thinking outside the brain by taking a look back at the time when the idea first emerged. In 1997, Andy Clark—then a professor of philosophy at Washington University in St. Louis, Missouri—left his laptop behind on a train.</p>
    
</div>
</body>
</html>