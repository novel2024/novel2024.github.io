<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/5.1.0/css/bootstrap.min.css">
</head>
<body>
<div class="container mt-5">
    
    <p> As we construct the concept map, the process may reveal gaps in our understanding of which we were previously unaware. And, having gone through the process of concept mapping,
we remember the material better—because we have thought deeply about its meaning. Once the concept map is completed, the knowledge that usually resides inside the head is made visible. By inspecting the map, we’re better able to see the big picture, and to resist becoming distracted by individual details.</p>
    
    <p> We can also more readily perceive how the different parts of a complex whole are related to one another.
Joseph Novak, now a professor emeritus of biology and science education at Cornell University, was investigating the way children learn science when he pioneered the concept-mapping method in the 1970s. Although the technique originated in education, Novak notes that it is increasingly being applied in the world of work—where, he says, “the knowledge structure necessary to understand and resolve problems is often an order of magnitude more complex” than that which is required in academic settings. Concept maps can vary enormously in size and complexity, from a simple diagram to an elaborate plan featuring hundreds of interacting elements.</p>
    
    <p>
Robert Caro’s map, for example, is big: big enough to stand in front of, to walk along, to lean into and stand back from. The sheer expansiveness of his outline allows Caro to bring to bear on his project not only his purely cognitive faculties of reasoning and analysis but also his more visceral powers of navigation and wayfinding. Researchers are now producing evidence that these ancient evolved capacities can help us to think more intelligently about abstract concepts—an insight that showed up first in, of all places, a futuristic action film.
 THE SCENE FROM the 2002 movie Minority Report is famous because, well, it’s just so cool: Chief of Precrime John Anderton, played by Tom Cruise, stands in front of a bank of gigantic computer screens.</p>
    
    <p> He is reviewing evidence of a crime yet to be committed, but this is no staid intellectual exercise; the way he interacts with the information splayed before him is active, almost tactile. He reaches out with his hands to grab and move images as if they were physical objects; he turns his head to catch a scene unfolding in his peripheral vision; he takes a step forward to inspect a picture more closely. Cruise, as Anderton,
physically navigates through the investigative file as he would through a three- dimensional landscape.
The movie, based on a short story by Philip K.</p>
    
    <p> Dick and set in the year 2054,
featured technology that was not yet available in the real world—yet John Anderton’s use of the interface comes off as completely plausible, even (to him) unexceptional. David Kirby, a professor of science, technology, and society at California Polytechnic State University, maintains that this is the key to moviegoers’ suspension of disbelief. “The most successful cinematic technologies are taken for granted by the characters” in a film, he writes, “and thus, communicate to the audience that these are not extraordinary but rather everyday technologies.” The director of Minority Report, Steven Spielberg, had an important factor working in his favor when he staged this scene.</p>
    
    <p> The technology employed by his lead character relied on a human capacity that could hardly be more “everyday” or “taken for granted”: the ability to move ourselves through space. For added verisimilitude, Spielberg invited computer scientists from the Massachusetts Institute of Technology to collaborate on the film’s production, encouraging them “to take on that design work as if it were an R&D effort,” says John Underkoffler, one of the researchers from MIT. And in a sense, it was: following the release of the movie, Underkoffler says, he was approached by “countless” investors and CEOs who wanted to know “Is that real? Can we pay you to build it if it’s not real?” Since then, scientists have succeeded at building something quite similar to the technology that Tom Cruise engaged to such dazzling effect. (John Underkoffler is now himself the CEO of Oblong Industries, developer of a Minority Report–like user interface he calls a Spatial Operating Environment.</p>
    
    <p>) What’s more, researchers have begun to study the cognitive effects of this technology, and they find that it makes real a promise of science fiction: it helps people to think more intelligently.
The particular tool that has become the subject of empirical investigation is the “large high-resolution display”—an oversized computer screen to which users can bring some of the same navigational capacities they would apply to a real-world landscape. Picture a bank of computer screens three and a half feet wide and nine feet long, presenting to the eye some 31.5 million pixels (the average computer monitor has fewer than 800,000 pixels).</p>
    
    <p> Robert Ball, an associate professor of computer science at Weber State University in Utah, has run numerous studies comparing people’s performance when interacting with a display like this to their performance when consulting a conventionally proportioned screen.
The improvements generated by the use of the super-sized display are striking.
Ball and his collaborators have reported that large high-resolution displays increase by more than tenfold the average speed at which basic visualization tasks are completed. On more challenging tasks, such as pattern finding, study participants improved their performance by 200 to 300 percent when using large displays.</p>
    
</div>
</body>
</html>